# Результаты домашнего задания "10.06. Инцидент-менеджмент"

## Задание 1

Составьте постмотрем, на основе реального сбоя системы Github в 2018 году.

Информация о сбое доступна [в виде краткой выжимки на русском языке](https://habr.com/ru/post/427301/) , а
также [развёрнуто на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).

---

<table>
    <colgroup>
        <col width="240px" style="background-color: DarkSlateGrey">
    </colgroup>
    <tr>
        <td><strong>Краткое описание инцидента</strong></td>
        <td>В 22:52 21 октября 2018 были затронуты некоторые службы GitHub в связи с проблемой на сетевых разделах и последущего сбоя на БД. В результатае возникла несогласованность информации на сервисах (данных). Произошло ухудшение сервиса в тесении 24 часов и 11 минут</td>
    </tr>
    <tr>
        <td><strong>Предшествующие события</strong></td>
        <td>Регламентные работы по замене оборудования</td>
    </tr>
     <tr>
        <td><strong>Причина инцидента</strong></td>
        <td>В результате регламентных работ по замене вышедшего из строя оптического оборудования 100G была потеряна связь между нашим сетевым концентратором на восточном побережье США и нашим основным центром обработки данных на восточном побережье США</td>
    </tr>
    <tr>
        <td><strong>Воздействие</strong></td>
        <td>Несогласованность предоставляемой информации (отображение не актуальной информации). Невозможность работы событий WebHook и недоступность работы страниц GitHub</td>
    </tr>
    <tr>
        <td><strong>Обнаружение</strong></td>
        <td>Обнаружено инженерами обрабтывающими алерты монторинга</td>
    </tr>
    <tr>
        <td><strong>Реакция</strong></td>
        <td>Связь между этими точками была восстановлена ​​за 43 секунды, но этот кратковременный сбой вызвал цепочку событий, которые привели к ухудшению качества обслуживания на 24 часа 11 минут. Что привело к рассогласованности кластеров MySQL</td>
    </tr>
    <tr>
        <td><strong>Восстановление</strong></td>
        <td>Восстановление полной производительности было выполнено за счет восстановления данных из бэкапов и повторных репликаций всех имеющихся данных с хостов с актуальными данными для восстановления 100% целостности данных во всех кластерах хранения данных</td>
    </tr>
    <tr>
        <td rowspan="15"><strong>Таймлайн</strong></td>
    </tr>
    <tr>
        <td>2018.10.21 22:52 UTC - потеря консенсуса между серверами в дата хабах в результате описанного инцидента. После восстановления была попытка восстановления целостности кластера, восстановления консенсуса, но данные в БД различялись что привело к несогласованности в рамках кластера</td>
    </tr>
    <tr>
        <td>2018.10.21 22:54 UTC - мониторинг генерировал предупреждения, инженера поддержки отвечали и обрабатывали входящие сообщения, в 23:02 обнаружили несоответсвие статуса кластера. Выявлено отсутсвие серверов из US East Coast</td>
    </tr>
    <tr>
        <td>2018.10.21 23:07 UTC - отключены внутренние инструменны развертывания для предотвращения дополнительных имзменй. Сайт переведен в желтый статус и автомтически зафикисрован инциден в системе управления сбоями </td>
    </tr>
    <tr>
        <td>2018.10.21 23:13 UTC - после выявления воздействия на множественные сервера, выделены дополнительные инженера, выполнены действия для сохранения пользоательских данных, но деградация системы не была остановлена</td>
    </tr>
    <tr>
        <td>2018.10.21 23:19 UTC - были остановлены некоторые процессы (принудительная деградация системы) с целью повышения скорости восстановления</td>
    </tr>
    <tr>
        <td>2018.10.22 00:05 UTC - разработка плана по восстановлению системы и синхронизациям репликаций данных. Обновлен статус, чтобы сообщить пользователям, что мы собираемся выполнить контролируемую отработку отказа внутренней системы хранения данных</td>
    </tr>
    <tr>
        <td>2018.10.22 00:41 UTC - запущен процесс бэкапирования данных, мониторинг состояния работ</td>
    </tr>
    <tr>
        <td>2018.10.22 06:51 UTC - бэкапы выаолнены US East Coast data center и запущенно реплецирование с серверов в West Coast</td>
    </tr>
    <tr>
        <td>2018.10.22 07:46 UTC - опубликована расширенная информация для пользователей</td>
    </tr>
    <tr>
        <td>2018.10.22 11:12 UTC - востановлены сервера в US East Coast, продолжается реплицирование, наблюдается повышенная нагрузка при реплицировании</td>
    </tr>
    <tr>
        <td>2018.10.22 13:15 UTC - приближались к пиковому периоду нагрузок. Увеличили количество репликаций для снятия растущей нагрузки по реклицированию</td>
    </tr>
    <tr>
        <td>2018.10.22 16:24 UTC - реплицированиее синхронизировано, переключение в штатную топологию MySQL</td>
    </tr>
    <tr>
        <td>2018.10.22 16:45 UTC - после восстановления возникла необходимость балансировки нагрузки для восстановления 100% услуг клиентам. Для восстановления уже имеющихся данных пользователей включили обработку, так же подняли TTL до полного завершения и возвращения к штатной работе.</td>
    </tr>
     <tr>
        <td>2018.10.22 23:03 UTC - Работа возвращена к штатному режиму</td>
    </tr>
     <tr>
        <td><strong>Последующие действия</strong></td>
        <td>Собранны логи по всем серверам подвергнутых сбоям, производится анализ логов для выявления запросов которые требуется обработать вручну и информировать пользователей о возможных проблемах<br/>
        Запланированы иннициативы:
        <br/>
        Настройка конфигурациии Оркестратора для снижения рисков поведения вызвавшего инцидент
        <br/>
        Изменить скорость сбора информации для эффективного взаимодествия
        <br/>
        Запланировано изменение работы систем и цодов для возможности сохранения работоспособности при полной потере одного из цодов в полнофункциональнос состоянии в том числе в переход на режим active/active/active
        <br/>
        Так же запланирован ряд других иннициатив для повышения ээфективности работы системыи и снижения риска потери работоспособности системы</td>
    </tr>
</table>
